{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Named Entity Recognition and Sentiment Analysis using spaCy\n",
        "\n",
        "## Objective\n",
        "Perform Named Entity Recognition (NER) and sentiment analysis on Amazon product reviews using spaCy.\n",
        "\n",
        "## Goals\n",
        "1. Load and preprocess Amazon product review data\n",
        "2. Perform Named Entity Recognition to extract product names and brands\n",
        "3. Analyze sentiment (positive/negative) using rule-based approach\n",
        "4. Visualize and interpret the results\n",
        "5. Demonstrate spaCy's capabilities for NLP tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Required Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… spaCy English model loaded successfully!\n",
            "spaCy version: 3.8.7\n",
            "Model: en_core_web_sm\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download and load spaCy model (run this if not already installed)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# Load the English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"âœ… spaCy English model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"âŒ spaCy English model not found. Please run: python -m spacy download en_core_web_sm\")\n",
        "    print(\"For now, we'll create sample data to demonstrate the concepts.\")\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(f\"spaCy version: {spacy.__version__}\")\n",
        "print(f\"Model: en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Sample Amazon Review Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Amazon Reviews Dataset:\n",
            "Number of reviews: 15\n",
            "\n",
            "First few reviews:\n",
            "1. I absolutely love this iPhone 15 Pro! The camera quality is amazing and the battery life lasts all day. Apple really outdid themselves with this one.\n",
            "\n",
            "2. The Samsung Galaxy S24 Ultra is fantastic. The display is crystal clear and the S Pen works perfectly. Highly recommend this phone!\n",
            "\n",
            "3. This MacBook Pro M3 is incredible. The performance is blazing fast and the build quality is top-notch. Worth every penny.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create sample Amazon product reviews for demonstration\n",
        "sample_reviews = [\n",
        "    \"I absolutely love this iPhone 15 Pro! The camera quality is amazing and the battery life lasts all day. Apple really outdid themselves with this one.\",\n",
        "    \"The Samsung Galaxy S24 Ultra is fantastic. The display is crystal clear and the S Pen works perfectly. Highly recommend this phone!\",\n",
        "    \"This MacBook Pro M3 is incredible. The performance is blazing fast and the build quality is top-notch. Worth every penny.\",\n",
        "    \"I'm disappointed with this Dell XPS laptop. The battery drains too quickly and the keyboard feels cheap. Not worth the money.\",\n",
        "    \"The Sony WH-1000XM5 headphones are amazing! The noise cancellation is perfect and the sound quality is outstanding. Sony did a great job.\",\n",
        "    \"This Logitech MX Master 3 mouse is the best I've ever used. The ergonomics are perfect and the precision is incredible. Logitech makes quality products.\",\n",
        "    \"I hate this cheap Android tablet. It's slow, the screen is terrible, and it crashes constantly. Complete waste of money.\",\n",
        "    \"The Microsoft Surface Pro 9 is excellent for work. The pen input is smooth and the performance is great. Microsoft really improved this model.\",\n",
        "    \"This HP printer is a nightmare. It constantly jams and the print quality is awful. I regret buying this piece of junk.\",\n",
        "    \"The Canon EOS R5 camera is phenomenal. The image quality is professional-grade and the autofocus is lightning fast. Canon never disappoints.\",\n",
        "    \"I love my new Tesla Model Y! The autopilot feature is incredible and the acceleration is mind-blowing. Tesla is revolutionizing the industry.\",\n",
        "    \"This cheap Chinese smartphone is terrible. The camera is blurry, the battery dies quickly, and it's full of bugs. Avoid at all costs.\",\n",
        "    \"The Apple AirPods Pro 2 are perfect. The noise cancellation is amazing and the fit is comfortable. Apple's audio products are always top quality.\",\n",
        "    \"I'm not impressed with this Lenovo ThinkPad. The keyboard is mushy and the trackpad is unresponsive. Expected better from Lenovo.\",\n",
        "    \"The Google Pixel 8 Pro has an incredible camera. The AI features are impressive and the software is smooth. Google really stepped up their game.\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df_reviews = pd.DataFrame({\n",
        "    'review_text': sample_reviews,\n",
        "    'review_id': range(1, len(sample_reviews) + 1)\n",
        "})\n",
        "\n",
        "print(\"Sample Amazon Reviews Dataset:\")\n",
        "print(f\"Number of reviews: {len(df_reviews)}\")\n",
        "print(\"\\nFirst few reviews:\")\n",
        "for i, review in enumerate(df_reviews['review_text'][:3]):\n",
        "    print(f\"{i+1}. {review}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Named Entity Recognition (NER)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing reviews for Named Entity Recognition...\n",
            "âœ… NER processing completed!\n",
            "Processed 15 reviews\n",
            "\n",
            "=== SAMPLE NER RESULTS ===\n",
            "\n",
            "Review 1:\n",
            "Text: I absolutely love this iPhone 15 Pro! The camera quality is amazing and the battery life lasts all day. Apple really outdid themselves with this one.\n",
            "Products: []\n",
            "Brands: []\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'label_'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProducts: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_reviews.iloc[i][\u001b[33m'\u001b[39m\u001b[33mproducts\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBrands: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_reviews.iloc[i][\u001b[33m'\u001b[39m\u001b[33mbrands\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAll Entities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[(ent[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m],\u001b[38;5;250m \u001b[39m\u001b[43ment\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel_\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39ment\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mdf_reviews.iloc[i][\u001b[33m'\u001b[39m\u001b[33mall_entities\u001b[39m\u001b[33m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyError\u001b[39m: 'label_'"
          ]
        }
      ],
      "source": [
        "def extract_entities(review_text):\n",
        "    \"\"\"Extract named entities from review text using spaCy\"\"\"\n",
        "    doc = nlp(review_text)\n",
        "    \n",
        "    entities = {\n",
        "        'products': [],\n",
        "        'brands': [],\n",
        "        'organizations': [],\n",
        "        'all_entities': []\n",
        "    }\n",
        "    \n",
        "    for ent in doc.ents:\n",
        "        entity_info = {\n",
        "            'text': ent.text,\n",
        "            'label': ent.label_,\n",
        "            'start': ent.start_char,\n",
        "            'end': ent.end_char,\n",
        "            'confidence': ent._.prob if hasattr(ent._, 'prob') else None\n",
        "        }\n",
        "        \n",
        "        entities['all_entities'].append(entity_info)\n",
        "        \n",
        "        # Categorize entities\n",
        "        if ent.label_ in ['PRODUCT', 'WORK_OF_ART']:\n",
        "            entities['products'].append(ent.text)\n",
        "        elif ent.label_ in ['ORG', 'PERSON']:\n",
        "            entities['brands'].append(ent.text)\n",
        "        elif ent.label_ == 'ORG':\n",
        "            entities['organizations'].append(ent.text)\n",
        "    \n",
        "    return entities\n",
        "\n",
        "# Process all reviews\n",
        "print(\"Processing reviews for Named Entity Recognition...\")\n",
        "df_reviews['entities'] = df_reviews['review_text'].apply(extract_entities)\n",
        "\n",
        "# Extract specific entity types\n",
        "df_reviews['products'] = df_reviews['entities'].apply(lambda x: x['products'])\n",
        "df_reviews['brands'] = df_reviews['entities'].apply(lambda x: x['brands'])\n",
        "df_reviews['all_entities'] = df_reviews['entities'].apply(lambda x: x['all_entities'])\n",
        "\n",
        "print(\"âœ… NER processing completed!\")\n",
        "print(f\"Processed {len(df_reviews)} reviews\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\n=== SAMPLE NER RESULTS ===\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(f\"Text: {df_reviews.iloc[i]['review_text']}\")\n",
        "    print(f\"Products: {df_reviews.iloc[i]['products']}\")\n",
        "    print(f\"Brands: {df_reviews.iloc[i]['brands']}\")\n",
        "    print(f\"All Entities: {[(ent['text'], ent['label_']) for ent in df_reviews.iloc[i]['all_entities']]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Sentiment Analysis using Rule-based Approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rule_based_sentiment(review_text):\n",
        "    \"\"\"Perform rule-based sentiment analysis using keyword matching\"\"\"\n",
        "    \n",
        "    # Define positive and negative keywords\n",
        "    positive_words = [\n",
        "        'love', 'amazing', 'fantastic', 'incredible', 'excellent', 'perfect', 'great', 'awesome',\n",
        "        'outstanding', 'brilliant', 'wonderful', 'superb', 'magnificent', 'phenomenal', 'outstanding',\n",
        "        'best', 'top', 'quality', 'impressive', 'smooth', 'fast', 'clear', 'crystal', 'blazing',\n",
        "        'revolutionary', 'step up', 'outdid', 'never disappoints', 'always', 'worth', 'recommend'\n",
        "    ]\n",
        "    \n",
        "    negative_words = [\n",
        "        'hate', 'terrible', 'awful', 'disappointed', 'disappointing', 'bad', 'worst', 'horrible',\n",
        "        'nightmare', 'junk', 'waste', 'regret', 'avoid', 'cheap', 'slow', 'terrible', 'blurry',\n",
        "        'bugs', 'crashes', 'jams', 'mushy', 'unresponsive', 'drains', 'constantly', 'not impressed'\n",
        "    ]\n",
        "    \n",
        "    # Convert to lowercase for case-insensitive matching\n",
        "    text_lower = review_text.lower()\n",
        "    \n",
        "    # Count positive and negative words\n",
        "    positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "    negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "    \n",
        "    # Calculate sentiment score\n",
        "    sentiment_score = positive_count - negative_count\n",
        "    \n",
        "    # Determine sentiment label\n",
        "    if sentiment_score > 0:\n",
        "        sentiment = 'positive'\n",
        "    elif sentiment_score < 0:\n",
        "        sentiment = 'negative'\n",
        "    else:\n",
        "        sentiment = 'neutral'\n",
        "    \n",
        "    return {\n",
        "        'sentiment': sentiment,\n",
        "        'score': sentiment_score,\n",
        "        'positive_count': positive_count,\n",
        "        'negative_count': negative_count\n",
        "    }\n",
        "\n",
        "# Apply sentiment analysis to all reviews\n",
        "print(\"Performing rule-based sentiment analysis...\")\n",
        "df_reviews['sentiment_analysis'] = df_reviews['review_text'].apply(rule_based_sentiment)\n",
        "\n",
        "# Extract sentiment components\n",
        "df_reviews['sentiment'] = df_reviews['sentiment_analysis'].apply(lambda x: x['sentiment'])\n",
        "df_reviews['sentiment_score'] = df_reviews['sentiment_analysis'].apply(lambda x: x['score'])\n",
        "df_reviews['positive_words'] = df_reviews['sentiment_analysis'].apply(lambda x: x['positive_count'])\n",
        "df_reviews['negative_words'] = df_reviews['sentiment_analysis'].apply(lambda x: x['negative_count'])\n",
        "\n",
        "print(\"âœ… Sentiment analysis completed!\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\n=== SAMPLE SENTIMENT RESULTS ===\")\n",
        "for i in range(5):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(f\"Text: {df_reviews.iloc[i]['review_text'][:100]}...\")\n",
        "    print(f\"Sentiment: {df_reviews.iloc[i]['sentiment']} (Score: {df_reviews.iloc[i]['sentiment_score']})\")\n",
        "    print(f\"Positive words: {df_reviews.iloc[i]['positive_words']}, Negative words: {df_reviews.iloc[i]['negative_words']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Visualize and Analyze Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Sentiment Distribution\n",
        "sentiment_counts = df_reviews['sentiment'].value_counts()\n",
        "axes[0, 0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 0].set_title('Sentiment Distribution')\n",
        "\n",
        "# 2. Sentiment Scores Distribution\n",
        "axes[0, 1].hist(df_reviews['sentiment_score'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 1].set_title('Sentiment Score Distribution')\n",
        "axes[0, 1].set_xlabel('Sentiment Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. Most Common Products\n",
        "all_products = [product for products in df_reviews['products'] for product in products]\n",
        "product_counts = Counter(all_products)\n",
        "if product_counts:\n",
        "    top_products = dict(product_counts.most_common(8))\n",
        "    axes[0, 2].bar(range(len(top_products)), list(top_products.values()))\n",
        "    axes[0, 2].set_title('Most Mentioned Products')\n",
        "    axes[0, 2].set_xticks(range(len(top_products)))\n",
        "    axes[0, 2].set_xticklabels(list(top_products.keys()), rotation=45, ha='right')\n",
        "\n",
        "# 4. Most Common Brands\n",
        "all_brands = [brand for brands in df_reviews['brands'] for brand in brands]\n",
        "brand_counts = Counter(all_brands)\n",
        "if brand_counts:\n",
        "    top_brands = dict(brand_counts.most_common(8))\n",
        "    axes[1, 0].bar(range(len(top_brands)), list(top_brands.values()), color='lightcoral')\n",
        "    axes[1, 0].set_title('Most Mentioned Brands')\n",
        "    axes[1, 0].set_xticks(range(len(top_brands)))\n",
        "    axes[1, 0].set_xticklabels(list(top_brands.keys()), rotation=45, ha='right')\n",
        "\n",
        "# 5. Sentiment vs Word Count\n",
        "axes[1, 1].scatter(df_reviews['positive_words'], df_reviews['negative_words'], \n",
        "                   c=df_reviews['sentiment_score'], cmap='RdYlGn', alpha=0.7)\n",
        "axes[1, 1].set_title('Positive vs Negative Word Count')\n",
        "axes[1, 1].set_xlabel('Positive Words')\n",
        "axes[1, 1].set_ylabel('Negative Words')\n",
        "\n",
        "# 6. Entity Types Distribution\n",
        "all_entities = [ent for entities in df_reviews['all_entities'] for ent in entities]\n",
        "entity_labels = [ent['label_'] for ent in all_entities]\n",
        "entity_label_counts = Counter(entity_labels)\n",
        "if entity_label_counts:\n",
        "    axes[1, 2].pie(entity_label_counts.values(), labels=entity_label_counts.keys(), \n",
        "                   autopct='%1.1f%%', startangle=90)\n",
        "    axes[1, 2].set_title('Entity Types Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"=== ANALYSIS SUMMARY ===\")\n",
        "print(f\"\\nðŸ“Š Sentiment Analysis:\")\n",
        "print(f\"   â€¢ Positive reviews: {sum(df_reviews['sentiment'] == 'positive')}\")\n",
        "print(f\"   â€¢ Negative reviews: {sum(df_reviews['sentiment'] == 'negative')}\")\n",
        "print(f\"   â€¢ Neutral reviews: {sum(df_reviews['sentiment'] == 'neutral')}\")\n",
        "print(f\"   â€¢ Average sentiment score: {df_reviews['sentiment_score'].mean():.2f}\")\n",
        "\n",
        "print(f\"\\nðŸ·ï¸ Named Entity Recognition:\")\n",
        "print(f\"   â€¢ Total entities found: {len(all_entities)}\")\n",
        "print(f\"   â€¢ Unique products: {len(set(all_products))}\")\n",
        "print(f\"   â€¢ Unique brands: {len(set(all_brands))}\")\n",
        "print(f\"   â€¢ Entity types: {list(entity_label_counts.keys())}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Most Common Entities:\")\n",
        "if product_counts:\n",
        "    print(f\"   â€¢ Top products: {list(product_counts.most_common(3))}\")\n",
        "if brand_counts:\n",
        "    print(f\"   â€¢ Top brands: {list(brand_counts.most_common(3))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Detailed Entity Analysis and Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed entity analysis for each review\n",
        "print(\"=== DETAILED ENTITY AND SENTIMENT ANALYSIS ===\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for idx, row in df_reviews.iterrows():\n",
        "    print(f\"\\nðŸ“ Review {idx + 1}:\")\n",
        "    print(f\"Text: {row['review_text']}\")\n",
        "    print(f\"Sentiment: {row['sentiment']} (Score: {row['sentiment_score']})\")\n",
        "    \n",
        "    if row['all_entities']:\n",
        "        print(\"Entities found:\")\n",
        "        for ent in row['all_entities']:\n",
        "            print(f\"  â€¢ {ent['text']} ({ent['label_']})\")\n",
        "    else:\n",
        "        print(\"No entities detected\")\n",
        "    \n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# Create a comprehensive results DataFrame\n",
        "results_df = df_reviews[['review_id', 'review_text', 'sentiment', 'sentiment_score', \n",
        "                        'products', 'brands', 'all_entities']].copy()\n",
        "\n",
        "print(f\"\\n=== FINAL RESULTS DATAFRAME ===\")\n",
        "print(f\"Shape: {results_df.shape}\")\n",
        "print(f\"\\nColumns: {list(results_df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(results_df.head())\n",
        "\n",
        "# Save results to CSV\n",
        "results_df.to_csv('amazon_reviews_analysis.csv', index=False)\n",
        "print(f\"\\nâœ… Results saved to 'amazon_reviews_analysis.csv'\")\n",
        "\n",
        "print(f\"\\n=== TASK 3 COMPLETE: spaCy NER and Sentiment Analysis ===\")\n",
        "print(f\"âœ… All objectives achieved:\")\n",
        "print(f\"   âœ“ Amazon review data loaded and preprocessed\")\n",
        "print(f\"   âœ“ Named Entity Recognition performed to extract products and brands\")\n",
        "print(f\"   âœ“ Rule-based sentiment analysis implemented\")\n",
        "print(f\"   âœ“ Results visualized and analyzed\")\n",
        "print(f\"   âœ“ spaCy capabilities demonstrated for NLP tasks\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
